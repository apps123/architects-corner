## My Architect's Corner

A collection of interesting technologies 

### Security, Authentication and Authorization

  * MTLS
  * Oauth2
  * Open ID
  * Crypto keys and signed certifcates
  * Two factor authentication
  * SAML

### AWS Cloud
    Cloud Formation
    S3
    EC2
    SQS/Kinesis Queue

### Java

| Java Version | Features released in JDK X | 
|:--------------|:----------------------------|
| JDK 8 and JDK 9 | [https://www.javatpoint.com/New-features-in-java](https://www.javatpoint.com/New-features-in-java) |
| JDK 8 and 9 | [https://static.rainfocus.com/oracle/oraclecode17/sess/1485992822413001Yd6N/PF/Cool%20in%20Java%208,%20and%20new%20in%20Java%209.pdf](https://static.rainfocus.com/oracle/oraclecode17/sess/1485992822413001Yd6N/PF/Cool%20in%20Java%208,%20and%20new%20in%20Java%209.pdf) |
| JDK 9 | [https://docs.oracle.com/javase/9/whatsnew/JSNEW.pdf](https://docs.oracle.com/javase/9/whatsnew/JSNEW.pdf) |
| JDK 9 | [http://www.sdjug.org/docs/2017-04-18-Java9.pdf](http://www.sdjug.org/docs/2017-04-18-Java9.pdf) |
| 55 New features in JDK9 | [https://www.doag.org/formes/pubfiles/9504016/2017-Java-Simon_Ritter-55_New_Features_in_JDK_9-Praesentation.pdf](https://www.doag.org/formes/pubfiles/9504016/2017-Java-Simon_Ritter-55_New_Features_in_JDK_9-Praesentation.pdf) |
| | |
| JDK 10 | [https://www.oracle.com/technetwork/cn/community/developer-day/2-55-new-features-java-se-8-2202551-zhs.pdf](https://www.oracle.com/technetwork/cn/community/developer-day/2-55-new-features-java-se-8-2202551-zhs.pdf) |
| *JDK 10* | [http://enos.itcollege.ee/~jpoial/allalaadimised/reading/Java-8-Features.pdf](http://enos.itcollege.ee/~jpoial/allalaadimised/reading/Java-8-Features.pdf) |
| | |
| JDK 9, 10 and 11 | [https://assets.azul.com/files/JDK-9-10-11-and-Beyond.pdf](https://assets.azul.com/files/JDK-9-10-11-and-Beyond.pdf) |
| JDK 10 | [https://developer.oracle.com/devo/res/pdf/1385446602743/Oracle-Java10.pdf](https://developer.oracle.com/devo/res/pdf/1385446602743/Oracle-Java10.pdf) |
| JDK 10 | [https://www.baeldung.com/java-10-overview](https://www.baeldung.com/java-10-overview) |
| JDK 10 | [https://howtodoinjava.com/java10/java10-features/](https://howtodoinjava.com/java10/java10-features/) |
| | |
| JDK 11| [https://dzone.com/articles/90-new-features-and-apis-in-jdk-11](https://dzone.com/articles/90-new-features-and-apis-in-jdk-11) |
| JDK 11| [https://blog.overops.com/java-11-will-include-more-than-just-features/](https://blog.overops.com/java-11-will-include-more-than-just-features/) |
| | |

### Java Stacks

  * [https://opensource.google.com/projects/list/featured](https://opensource.google.com/projects/list/featured)
  * [https://medium.com/issuehunt/top-11-popular-java-projects-on-github-48aaad5b4e0a](https://medium.com/issuehunt/top-11-popular-java-projects-on-github-48aaad5b4e0a)
  * [https://www.apache.org/index.html#projects-list](https://www.apache.org/index.html#projects-list)
  * [https://medium.com/issuehunt/50-top-java-projects-on-github-adbfe9f67dbc](https://medium.com/issuehunt/50-top-java-projects-on-github-adbfe9f67dbc)
  * [https://www.javaworld.com/blog/open-source-java-projects/](https://www.javaworld.com/blog/open-source-java-projects/)
  * [https://java-source.net/](https://java-source.net/)
  * [https://24pullrequests.com/projects](https://24pullrequests.com/projects)



| Name | Description | Link |
|:-----|:------------|:-----|
|Tellirium|Tellurium Automated Testing Framework is an open source automated testing framework for testing web applications. Tellurium evolved from Selenium framework around 2017 with a different testing approach. Tellurium is built on UI module concept, which makes it possible to write reusable and easy to maintain tests against the dynamic RIA based web applications.|[http://www.methodsandtools.com/tools/tools.php?tellurium](http://www.methodsandtools.com/tools/tools.php?tellurium)|  
| Lambda Behave | Support Spec like output to JUnit uses lamda support of JDK 8| [https://github.com/RichardWarburton/lambda-behave](https://github.com/RichardWarburton/lambda-behave) |
| HierarchicalContextRunner | Support Spec like output using JUnit with lambda support of JDK 8 | [https://github.com/bechte/junit-hierarchicalcontextrunner/wiki](https://github.com/bechte/junit-hierarchicalcontextrunner/wiki) |
| | | |
| | | |

### Scala


### Spark


### Big Data


### Machine Learning


### Algorithms


### Data Structures


### Java Opensource Software


### GOF Software Design Patterns


### System Design 

  * [https://github.com/donnemartin/system-design-primer](https://github.com/donnemartin/system-design-primer)

### Web Service Architecture Design Patterns

  * Microservices Design Patterns [https://www.slideshare.net/JavaDayUA/microservices-design-patterns-for-java-application](https://www.slideshare.net/JavaDayUA/microservices-design-patterns-for-java-application)

### Cloud Design Patterns

  * Azure Cloud Design Patterns [https://docs.microsoft.com/en-us/azure/architecture/patterns/](https://docs.microsoft.com/en-us/azure/architecture/patterns/)

### UML

  * UML reference - [https://holub.com/uml/](https://holub.com/uml/)

### Patterns in Software Engineering

  * Software Engineering Patterns - [http://sharif.edu/~ramsin/index_files/pselecture15.pdf](http://sharif.edu/~ramsin/index_files/pselecture15.pdf)

### Relational DB


### Common Architect's Software stacks

  * Queues
  * NoSQL
  * Solr
  * Elasticsearch
  * Redis
  * Lucene
  * Docker
  * Kubernetes
  * Jenkins
  * GraphQL
  * Logstash
  

### Everyone needs a Job

| Source | Link |
|:-------|:-----|
| Javapoint | [https://www.javatpoint.com/interview-questions-and-answers](https://www.javatpoint.com/interview-questions-and-answers) |
| Behavioural Interview Questions | [https://medium.com/swlh/the-secret-to-answering-behavioral-interview-questions-68d13b4c625d](https://medium.com/swlh/the-secret-to-answering-behavioral-interview-questions-68d13b4c625d) |
| Grading rubric | [https://medium.engineering/engineering-interviews-grading-rubric-8b409bec021f](https://medium.engineering/engineering-interviews-grading-rubric-8b409bec021f) |
| Ultimate guide - freecodecamp.org | [https://www.freecodecamp.org/news/the-ultimate-guide-to-preparing-for-the-coding-interview-183251ee36c9/](https://www.freecodecamp.org/news/the-ultimate-guide-to-preparing-for-the-coding-interview-183251ee36c9/) |
| How I got into Google | [https://blog.usejournal.com/how-i-got-into-google-161c97913b8b](https://blog.usejournal.com/how-i-got-into-google-161c97913b8b) |
| Best Interview Questions | [https://beamery.com/blog/best-interview-questions](https://beamery.com/blog/best-interview-questions) |
| Worst Interview questions | [https://magazine.vunela.com/the-5-worst-interview-questions-and-how-to-answer-them-like-a-pro-2754cb58f2ac](https://magazine.vunela.com/the-5-worst-interview-questions-and-how-to-answer-them-like-a-pro-2754cb58f2ac) |
| Leetcode solutions to Interview questions | [https://github.com/fishercoder1534/Leetcode](https://github.com/fishercoder1534/Leetcode) |

### Steps to build datascience stack for Grid

  * Host: Find a RHEL7 host

  * Install docker on RHEL7
```
sudo yum install docker
docker ps -a
sudo systemctl enable docker.service
sudo systemctl start docker.service
sudo docker pull <registry>:4443/<hadoop>/rhel7
```
  * Steps

```
exit
sudo docker container start 798db503d5d9 # replace id
sudo docker exec -it  798db503d5d9 /bin/bash # replace id

pip3 install --upgrade pip

pip install virtualenv
pip install virtualenvwrapper

# Check ls -al /opt/python/bin/python -> python3.6
export WORKON_HOME=~/
export PYTHONHOME=/opt/python/
export PATH=$PYTHONHOME/bin:$PATH
export LD_LIBRARY_PATH=$PYTHONHOME/lib/:$PYTHONHOME/lib/python3.6/lib-dynload/:$LD_LIBRARY_PATH
export PYTHONPATH=$PYTHONHOME/lib/python3.6/:$PYTHONHOME/lib/python3.6/site-packages:$PYTHONPATH
export VIRTUALENVWRAPPER_PYTHON=$PYTHONHOME/bin/python


source $PYTHONHOME/bin/virtualenvwrapper.sh


mkvirtualenv datascience
deactivate
virtualenv --always-copy --relocatable --download /root/datascience
# Creates under /root/datascience
source /root/datascience/bin/activate

# verify /root/datascience/lib/python3.6/site-packages/../no-global-site-packages.txt

pip3 install matplotlib
pip3 install scipy
pip3 install scikit-learn
pip3 install pandas
pip3 install sklearn_pandas
pip3 install nltk
pip3 install seaborn
pip3 install bokeh
pip3 install plotly
pip3 install gensim
pip3 install statsmodels==0.6.0
pip3 install pydot
pip3 install XGBoost
pip3 install LightGBM
pip3 install CatBoost
pip3 install Eli5
pip3 install torch
pip3 install elephas
pip3 install keras
pip3 install theano
pip3 install tensorflow
pip3 install dist-keras
pip3 install ggplot
pip3 install virtualenv
pip3 install virtualenvwrapper

 cp /opt/python/lib/*.so lib/
 cp -rf  /opt/python/lib/python3.6/* /root/datascience/lib/python3.6/
 cp -rf  /opt/python/lib/python3.6/lib-dynload/* /root/datascience/lib/python3.6/lib-dynload/

export PYTHONHOME=/root/datascience
export PATH=$PYTHONHOME/bin:$PATH
export LD_LIBRARY_PATH=$PYTHONHOME/lib/:$PYTHONHOME/lib/python3.6/lib-dynload/:$LD_LIBRARY_PATH
export PYTHONPATH=$PYTHONHOME/lib/python3.6/:$PYTHONHOME/lib/python3.6/site-packages:$PYTHONPATH


exit #from docker
# copy the label files
sudo docker cp labeled.tsv thirsty_lewin:/ #replace docker name, copy labeled TSV file
sudo docker exec -it  3ce98e47fe41 /bin/bash #replace id
mv labeled.tsv /root/datascience/
source /root/datascience/bin/activate

export PYTHONHOME=/root/datascience
export PATH=$PYTHONHOME/bin:$PATH
export LD_LIBRARY_PATH=$PYTHONHOME/lib/:$PYTHONHOME/lib/python3.6/lib-dynload/:$LD_LIBRARY_PATH
export PYTHONPATH=$PYTHONHOME/lib/python3.6/:$PYTHONHOME/lib/python3.6/site-packages:$PYTHONPATH

cd /root/datascience
python lr.py # build model
mkdir models
mv orders.mlmodel orders.vectorizer models
tar cvzf ../datascience.tar.gz *
exit
sudo docker cp  thirsty_lewin:/root/datascience/datascience.tar.gz .


```

## Hadoop Streaming 

```
hadoop jar /<path to jar>/hadoop-streaming.jar \
-Dmapreduce.map.env=YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=<hadoop distribution> \
-Dmapreduce.reduce.env=YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=<hadoop distribution> \
-Dyarn.app.mapreduce.am.env=YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=<hadoop distribution> \
-Dmapreduce.job.name="classification_ML" \
-Dmapreduce.job.reduces=500 \
-Dmapreduce.job.acl-view-job=* \
-Dmapreduce.job.acl-modify-job=* \
-Dmapred.input.format.class=org.apache.hadoop.mapred.TextInputFormat \
-Dstream.map.input.writer.class=org.apache.hadoop.streaming.io.TypedBytesInputWriter \
-Dstream.map.output.reader.class=org.apache.hadoop.streaming.io.TextOutputReader \
-Dmapred.job.map.memory.mb=4096 \
-Dmapreduce.reduce.memory.mb=8192 \
-Dmapreduce.task.io.sort.mb=128 \
-Dmapreduce.map.java.opts="-server -Xmx3072m -Djava.net.preferIPv4Stack=true" \
-jt <jobtracker_server_name>:8032 \
-fs hdfs://<namenode_server_name>:8020 \
-archives hdfs://<namenode_server_name>:8020/<path_to_distributed_cache>/datascience.tar.gz#datascience \
-cmdenv NLTK_DATA=./datascience/nltk_data \
-cmdenv PYTHON_EGG_CACHE=/tmp \
-cmdenv PYTHONHOME=./datascience/ \
-cmdenv PATH=./datascience/bin:$PATH \
-cmdenv PYTHONPATH=.:./datascience:./datascience/lib/python3.6:./datascience/lib/python3.6/site-packages/ \
-cmdenv LD_LIBRARY_PATH=.:./datascience/lib/:./datascience/lib/python3.6/lib-dynload/ \
-input hdfs://<namenode_server_name>:8020/<path_to_input>/input/* \
-output hdfs://<namenode_server_name>:8020/<path_to_output>/classified/ \
-mapper "/bin/cat" \
-reducer "./datascience/run.sh"
```

## Hadoop Streaming - keyed on the input

Uses key ordering and sorting on time, with python distributed cache

```
hadoop jar /<path to jar>/hadoop-streaming.jar \
-Dmapreduce.map.env=YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=<hadoop distribution> \
-Dmapreduce.reduce.env=YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=<hadoop distribution> \
-Dyarn.app.mapreduce.am.env=YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=<hadoop distribution> \
-Dmapreduce.job.name="some_job_name" \
-Dmapreduce.job.queuename=queuname \
-Dmapreduce.job.maps=100 \
-Dmapreduce.job.reduces=100 \
-Dmapreduce.job.acl-view-job=* \
-Dmapreduce.job.acl-modify-job=* \
-Dmapred.input.format.class=org.apache.hadoop.mapred.TextInputFormat \
-Dstream.map.input.writer.class=org.apache.hadoop.streaming.io.TypedBytesInputWriter \
-Dstream.map.output.reader.class=org.apache.hadoop.streaming.io.TextOutputReader \
-Dmapred.job.map.memory.mb=4096 \
-Dmapreduce.reduce.memory.mb=8192 \
-Dmapreduce.task.io.sort.mb=128 \
-Dmapreduce.map.java.opts="-server -Xmx3072m -Djava.net.preferIPv4Stack=true" \
-Dstream.num.map.output.key.fields=3 \
-Dmapreduce.partition.keypartitioner.options="-k1,1" \
-Dmapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \
-Dmapreduce.partition.keycomparator.options="-k1,1 -k3,3n" \
-jt <jobtracker_server_name>:8032 \
-fs hdfs://<namenode_server_name>:8020 \
-archives hdfs://<namenode_server_name>:8020/<path_to_distributed_cache>/datascience.tar.gz#datascience \
-cmdenv NLTK_DATA=./datascience/nltk_data \
-cmdenv PYTHON_EGG_CACHE=/tmp \
-cmdenv PYTHONHOME=./datascience/ \
-cmdenv PATH=./datascience/bin:$PATH \
-cmdenv PYTHONPATH=.:./datascience:./datascience/lib/python3.6:./datascience/lib/python3.6/site-packages/ \
-cmdenv LD_LIBRARY_PATH=.:./datascience/lib/:./datascience/lib/python3.6/lib-dynload/ \
-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \
-input hdfs://<namenode_server_name>:8020/<path_to_input>/* \
-output hdfs://<namenode_server_name>:8020/<path_to_output>/ \
-mapper "/bin/cat" \
-reducer "./datascience/metrics.sh"

```

